{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShrutiChrist/Natural-Language-Processing/blob/main/2348545_lab9(NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "559fb33a-2113-4f75-86b7-948d2f889c27",
      "metadata": {
        "id": "559fb33a-2113-4f75-86b7-948d2f889c27"
      },
      "source": [
        "## Edit Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "22a2bea6-7f7c-4b28-87b1-089501a474b7",
      "metadata": {
        "id": "22a2bea6-7f7c-4b28-87b1-089501a474b7"
      },
      "outputs": [],
      "source": [
        "def editDistance(str1, str2, m, n):\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    # If last characters of two strings are same, nothing\n",
        "    # much to do. Ignore last characters and get count for\n",
        "    # remaining strings.\n",
        "    if str1[m-1] == str2[n-1]:\n",
        "        return editDistance(str1, str2, m-1, n-1)\n",
        "\n",
        "    # If last characters are not same, consider all three\n",
        "    # operations on last character of first string, recursively\n",
        "    # compute minimum cost for all three operations and take\n",
        "    # minimum of three values.\n",
        "    return 1 + min(editDistance(str1, str2, m, n-1),    # Inser22\n",
        "                   editDistance(str1, str2, m-1, n),    # Remove\n",
        "                   editDistance(str1, str2, m-1, n-1)    # Replace\n",
        "                   )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "910422ac-0941-4d65-bf55-37bfacec763a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "910422ac-0941-4d65-bf55-37bfacec763a",
        "outputId": "363c3a51-5d30-4bb3-8a0b-3f18eb068e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter First String:shruti\n",
            "Enter Second String:shru\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "str1=input(\"Enter First String:\")\n",
        "str2=input(\"Enter Second String:\")\n",
        "print(editDistance(str1, str2, len(str1), len(str2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5baa15a0-b69e-4360-bff7-d82d9f008e70",
      "metadata": {
        "id": "5baa15a0-b69e-4360-bff7-d82d9f008e70"
      },
      "source": [
        "## Poem Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b244494d-304e-4223-9b19-e3d323707aae",
      "metadata": {
        "id": "b244494d-304e-4223-9b19-e3d323707aae"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8f40a759-689a-4f04-89c2-06d443ecd92e",
      "metadata": {
        "id": "8f40a759-689a-4f04-89c2-06d443ecd92e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.utils as ku\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398c39e3-a636-4430-bef3-b6b53206fbc2",
      "metadata": {
        "id": "398c39e3-a636-4430-bef3-b6b53206fbc2"
      },
      "source": [
        "### Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3c9b4a40-6344-492a-8b1d-f3e8d6b0f5d2",
      "metadata": {
        "id": "3c9b4a40-6344-492a-8b1d-f3e8d6b0f5d2",
        "outputId": "786e77e8-c39f-4ee9-810c-ae2aaad1a505",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 197\n"
          ]
        }
      ],
      "source": [
        "data = open('/content/poem.txt', encoding=\"utf8\").read()\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index)\n",
        "\n",
        "print(\"Total Words:\", total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50dd3cca-7417-40a2-ad57-54737bbf0400",
      "metadata": {
        "id": "50dd3cca-7417-40a2-ad57-54737bbf0400"
      },
      "source": [
        "### Text to Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "64c14f08-3b29-4552-9325-f15b48f1a7d2",
      "metadata": {
        "id": "64c14f08-3b29-4552-9325-f15b48f1a7d2"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "label = ku.to_categorical(label, num_classes=total_words+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "837dd1e5-781e-4979-a2b5-885bdcec52d9",
      "metadata": {
        "id": "837dd1e5-781e-4979-a2b5-885bdcec52d9"
      },
      "source": [
        "### Bi-directional LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fa90944d-9b28-4b1f-b2c3-9c003d9b8f41",
      "metadata": {
        "id": "fa90944d-9b28-4b1f-b2c3-9c003d9b8f41",
        "outputId": "714e8de5-0971-4b41-ca39-bbee3cb14b14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 11, 100)           19800     \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 11, 300)           301200    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 11, 300)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 197)               19897     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 198)               39204     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 540501 (2.06 MB)\n",
            "Trainable params: 540501 (2.06 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words+1, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words+1/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words+1, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "57276904-7921-477c-a3a1-22449b2fbd16",
      "metadata": {
        "id": "57276904-7921-477c-a3a1-22449b2fbd16",
        "outputId": "8a03ef1d-cb50-488c-c9c7-46d8a7064556",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 8s 59ms/step - loss: 6.4690 - accuracy: 0.0685\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 5.9837 - accuracy: 0.0804\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 5.6465 - accuracy: 0.0893\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 5.4750 - accuracy: 0.0804\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 5.3514 - accuracy: 0.0744\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 5.2254 - accuracy: 0.0714\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 5.0980 - accuracy: 0.0833\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 4.9802 - accuracy: 0.0744\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 4.8497 - accuracy: 0.0893\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 4.6982 - accuracy: 0.0774\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 4.5446 - accuracy: 0.1042\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 4.4394 - accuracy: 0.0952\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 4.3385 - accuracy: 0.1071\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 4.2129 - accuracy: 0.1161\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 4.1242 - accuracy: 0.0863\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 4.0468 - accuracy: 0.1161\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 4.0219 - accuracy: 0.1131\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 3.9222 - accuracy: 0.1161\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 3.8242 - accuracy: 0.0744\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 3.7559 - accuracy: 0.1339\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 3.7099 - accuracy: 0.1369\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 3.6724 - accuracy: 0.1369\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 3.6043 - accuracy: 0.1369\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 3.5574 - accuracy: 0.1786\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 1s 114ms/step - loss: 3.4914 - accuracy: 0.1607\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 1s 72ms/step - loss: 3.4157 - accuracy: 0.1696\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 3.3500 - accuracy: 0.1935\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 3.3592 - accuracy: 0.2202\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 3.3118 - accuracy: 0.1726\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 3.2629 - accuracy: 0.2113\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 3.2055 - accuracy: 0.2143\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 3.1517 - accuracy: 0.2024\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 3.1246 - accuracy: 0.2202\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 3.0266 - accuracy: 0.2321\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 2.9901 - accuracy: 0.2232\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 2.9281 - accuracy: 0.2381\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 2.9025 - accuracy: 0.2619\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.9016 - accuracy: 0.2411\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 2.8374 - accuracy: 0.2470\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.8148 - accuracy: 0.2530\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 1s 107ms/step - loss: 2.7771 - accuracy: 0.2917\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 1s 112ms/step - loss: 2.7540 - accuracy: 0.2827\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 2.7239 - accuracy: 0.2619\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.7071 - accuracy: 0.3006\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.6842 - accuracy: 0.2708\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 2.6380 - accuracy: 0.2798\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.6121 - accuracy: 0.2946\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.6352 - accuracy: 0.2738\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 2.6105 - accuracy: 0.3065\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.5597 - accuracy: 0.3155\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.5041 - accuracy: 0.3125\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 2.4910 - accuracy: 0.3363\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.4950 - accuracy: 0.3214\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 2.4908 - accuracy: 0.3333\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 2.5512 - accuracy: 0.3274\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.5209 - accuracy: 0.3095\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.4127 - accuracy: 0.3661\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 1s 96ms/step - loss: 2.3593 - accuracy: 0.3720\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 1s 113ms/step - loss: 2.2989 - accuracy: 0.3780\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 2.2714 - accuracy: 0.3929\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 2.2331 - accuracy: 0.4018\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.2015 - accuracy: 0.4226\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.1819 - accuracy: 0.4137\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.1829 - accuracy: 0.4375\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.1715 - accuracy: 0.4494\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 2.1344 - accuracy: 0.4464\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 2.1240 - accuracy: 0.4524\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.1930 - accuracy: 0.4494\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 2.2306 - accuracy: 0.4286\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.1390 - accuracy: 0.4048\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 2.0777 - accuracy: 0.4375\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.0475 - accuracy: 0.4940\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.0109 - accuracy: 0.5030\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.9621 - accuracy: 0.4970\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 1.9125 - accuracy: 0.5327\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 1.8936 - accuracy: 0.5506\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 1s 92ms/step - loss: 1.8929 - accuracy: 0.5536\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.8450 - accuracy: 0.5506\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.8254 - accuracy: 0.5417\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.8022 - accuracy: 0.5685\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.7739 - accuracy: 0.6220\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 1.7441 - accuracy: 0.5982\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 1.7164 - accuracy: 0.6250\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.6869 - accuracy: 0.6339\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.6620 - accuracy: 0.6458\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 1.6390 - accuracy: 0.6488\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.6204 - accuracy: 0.6548\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.6328 - accuracy: 0.6369\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.6617 - accuracy: 0.6220\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.6957 - accuracy: 0.6250\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.6539 - accuracy: 0.6399\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 1s 67ms/step - loss: 1.6331 - accuracy: 0.6607\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 1s 114ms/step - loss: 1.5994 - accuracy: 0.6429\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 1s 106ms/step - loss: 1.6348 - accuracy: 0.6280\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.5427 - accuracy: 0.6667\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.5057 - accuracy: 0.6726\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.4724 - accuracy: 0.7024\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.4178 - accuracy: 0.7232\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.4184 - accuracy: 0.7292\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.3889 - accuracy: 0.7500\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(predictors, label, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e177bff4-2995-4435-801b-3be1133fc04d",
      "metadata": {
        "id": "e177bff4-2995-4435-801b-3be1133fc04d"
      },
      "source": [
        "### Poem Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d8b15d42-e42a-44e0-ac9d-514ccb3654ca",
      "metadata": {
        "id": "d8b15d42-e42a-44e0-ac9d-514ccb3654ca",
        "outputId": "3b89dbce-a0e4-438b-b98d-daa8ded3b2e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter first few wordsnature\n",
            "nature in the heart of a forest where shadows dance with light light light light day day life strife light light light with light light light with gold with light light light with gold with gold with with gold with gold with with gold with with gold with with gold with\n"
          ]
        }
      ],
      "source": [
        "seed_text = input(\"Enter first few words\")\n",
        "next_words = 50\n",
        "ouptut_text = \"\"\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences(\n",
        "        [token_list], maxlen=max_sequence_len-1,\n",
        "      padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list,\n",
        "                                        verbose=0), axis=-1)\n",
        "    output_word = \"\"\n",
        "\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef8d433-ebad-4b91-a76f-39e5a6dda85f",
      "metadata": {
        "id": "6ef8d433-ebad-4b91-a76f-39e5a6dda85f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}