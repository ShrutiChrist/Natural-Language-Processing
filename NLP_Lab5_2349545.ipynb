{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAuB81yH/YGVBv7Eon3lDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShrutiChrist/Natural-Language-Processing/blob/main/NLP_Lab5_2349545.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYpsLLBt46hY",
        "outputId": "00137350-c483-41ba-8dba-032a79335771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Write a program to get Antonyms from WordNet.\n",
        "from nltk.corpus import wordnet\n",
        "def get_antonyms(word):\n",
        "    antonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            for antonym in lemma.antonyms():\n",
        "                antonyms.add(antonym.name())\n",
        "    return antonyms\n",
        "\n",
        "word = 'good'\n",
        "antonyms = get_antonyms(word)\n",
        "print(\"Antonyms of '{}' are: {}\".format(word, list(antonyms)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIohzwgi5rhb",
        "outputId": "1979d109-9fed-4b57-91d7-8943326e5af2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antonyms of 'good' are: ['badness', 'ill', 'evil', 'bad', 'evilness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Write a program for stemming non-English words.\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Create a Russian Snowball stemmer\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "# Example words to stem\n",
        "words = [\"понимающий\", \"говорящий\", \"читающий\"]  # Words in Russian\n",
        "\n",
        "# Stem the words\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDXAkEdq5urZ",
        "outputId": "094e277b-b1e8-4270-b5d2-328562ebb7fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['понима', 'говоря', 'чита']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Write a program for lemmatizing words Using WordNet (Use all type of stemmers for the comparison).\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "\n",
        "# Initialize stemmers\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Words to lemmatize and stem\n",
        "words = [\"running\", \"flies\", \"better\", \"included\"]\n",
        "\n",
        "# Lemmatize and stem each word\n",
        "for word in words:\n",
        "    print(\"Word:\", word)\n",
        "\n",
        "    # Lemmatization using WordNet\n",
        "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "    print(\"Lemma (WordNet):\", lemma)\n",
        "\n",
        "    # Stemming using Porter\n",
        "    porter_stem = porter_stemmer.stem(word)\n",
        "    print(\"Porter Stem:\", porter_stem)\n",
        "\n",
        "    # Stemming using Lancaster\n",
        "    lancaster_stem = lancaster_stemmer.stem(word)\n",
        "    print(\"Lancaster Stem:\", lancaster_stem)\n",
        "\n",
        "    # Stemming using Snowball\n",
        "    snowball_stem = snowball_stemmer.stem(word)\n",
        "    print(\"Snowball Stem:\", snowball_stem)\n",
        "\n",
        "    print(\"------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gsRtprH6Cwt",
        "outputId": "7146d5ef-8eb2-475d-e38b-29005d90d59e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: running\n",
            "Lemma (WordNet): run\n",
            "Porter Stem: run\n",
            "Lancaster Stem: run\n",
            "Snowball Stem: run\n",
            "------------------\n",
            "Word: flies\n",
            "Lemma (WordNet): fly\n",
            "Porter Stem: fli\n",
            "Lancaster Stem: fli\n",
            "Snowball Stem: fli\n",
            "------------------\n",
            "Word: better\n",
            "Lemma (WordNet): better\n",
            "Porter Stem: better\n",
            "Lancaster Stem: bet\n",
            "Snowball Stem: better\n",
            "------------------\n",
            "Word: included\n",
            "Lemma (WordNet): include\n",
            "Porter Stem: includ\n",
            "Lancaster Stem: includ\n",
            "Snowball Stem: includ\n",
            "------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Write a program to differentiate stemming and lemmatizing words.\n",
        "\n",
        "# Initialize a Porter Stemmer and a WordNet Lemmatizer\n",
        "porter_stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example words to differentiate\n",
        "words = [\"running\", \"flies\", \"better\", \"included\"]\n",
        "\n",
        "print(\"Original Words:\")\n",
        "for word in words:\n",
        "    print(word)\n",
        "\n",
        "print(\"\\nStemmed Words (Porter Stemmer):\")\n",
        "for word in words:\n",
        "    stemmed_word = porter_stemmer.stem(word)\n",
        "    print(stemmed_word)\n",
        "\n",
        "print(\"\\nLemmatized Words (WordNet Lemmatizer):\")\n",
        "for word in words:\n",
        "    lemmatized_word = lemmatizer.lemmatize(word, pos='v')\n",
        "    print(lemmatized_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD8krkMa6U2E",
        "outputId": "8f212b9f-4682-41fe-eb2a-5792a14b3614"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words:\n",
            "running\n",
            "flies\n",
            "better\n",
            "included\n",
            "\n",
            "Stemmed Words (Porter Stemmer):\n",
            "run\n",
            "fli\n",
            "better\n",
            "includ\n",
            "\n",
            "Lemmatized Words (WordNet Lemmatizer):\n",
            "run\n",
            "fly\n",
            "better\n",
            "include\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Write a program for PoS Tagging and also execute any of the tool that given in class.\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Prosessing includes topics like sentiment analysis,lemmitization,POS \"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Perform PoS tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "print(\"PoS tagging using NLTK:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# PoS tagging using SpaCY\n",
        "import spacy\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# Execute PoS tagging using SpaCy\n",
        "doc = nlp(text)\n",
        "print(\"\\nPoS tagging using SpaCy:\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiJ84gsm6wRw",
        "outputId": "3ad90d5e-a6e4-4131-bbf6-365e0c1db08f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PoS tagging using NLTK:\n",
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Prosessing', 'NNP'), ('includes', 'VBZ'), ('topics', 'NNS'), ('like', 'IN'), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('lemmitization', 'NN'), (',', ','), ('POS', 'NNP')]\n",
            "\n",
            "PoS tagging using SpaCy:\n",
            "Natural 96\n",
            "Language 96\n",
            "Prosessing 96\n",
            "includes 100\n",
            "topics 92\n",
            "like 85\n",
            "sentiment 92\n",
            "analysis 92\n",
            ", 97\n",
            "lemmitization 92\n",
            ", 97\n",
            "POS 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPE stands for Geopolitical Entity.It is a type"
      ],
      "metadata": {
        "id": "0fyLLuCvBCsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition using NLTK\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Sample text\n",
        "text = \"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Perform PoS tagging\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Perform Named Entity Recognition\n",
        "chunks = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "named_entities = []\n",
        "for chunk in chunks:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        named_entities.append(' '.join(c[0] for c in chunk))\n",
        "\n",
        "print(\"Named Entities using NLTK:\")\n",
        "print(named_entities)\n",
        "\n",
        "# Named Entity Recognition using SpaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Execute NER using SpaCy\n",
        "doc = nlp(text)\n",
        "print(\"\\nNamed Entities using SpaCy:\")\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNLyNLeD7AHE",
        "outputId": "8d647d78-ae1c-4417-848b-8e115ac56f28"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities using NLTK:\n",
            "['Indian', 'Space Research Organisation', 'India', 'Bengaluru', 'Department', 'Space', 'India', 'ISRO', 'DOS']\n",
            "\n",
            "Named Entities using SpaCy:\n",
            "The Indian Space Research Organisation ORG\n",
            "India GPE\n",
            "Bengaluru GPE\n",
            "Department of Space ORG\n",
            "India GPE\n",
            "ISRO ORG\n",
            "DOS ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency parsing focuses on the **linear structure** of the sentence and the relationships between words, while constituency parsing focuses on the **hierarchical structure** of the sentence."
      ],
      "metadata": {
        "id": "yI9TTdR5Fzhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependency parsing using SpaCy\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Perform dependency parsing using SpaCy\n",
        "doc = nlp(text)\n",
        "print(\"Dependency parsing using SpaCy:\")\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.dep_)\n",
        "\n",
        "# Constituency parsing using NLTK\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('treebank')\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform PoS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Perform constituency parsing using NLTK\n",
        "grammar = r\"\"\"\n",
        "  NP: {<DT>?<JJ>*<NN>}   # noun phrase\n",
        "  P: {<IN>}               # preposition\n",
        "  V: {<V.*>}              # verb\n",
        "  PP: {<P> <NP>}          # prepositional phrase\n",
        "  VP: {<V> <NP|PP>*}      # verb phrase\n",
        "\"\"\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(pos_tags)\n",
        "print(\"\\nConstituency parsing using NLTK:\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2Igm0gjDF_I",
        "outputId": "309be837-8bf9-4cad-f5ed-6f9f770fff7e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency parsing using SpaCy:\n",
            "The det fox nsubj\n",
            "quick amod fox nsubj\n",
            "brown amod fox nsubj\n",
            "fox nsubj jumps ROOT\n",
            "jumps ROOT jumps ROOT\n",
            "over prep jumps ROOT\n",
            "the det dog pobj\n",
            "lazy amod dog pobj\n",
            "dog pobj over prep\n",
            ". punct jumps ROOT\n",
            "\n",
            "Constituency parsing using NLTK:\n",
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  (VP (V jumps/VBZ) (PP (P over/IN) (NP the/DT lazy/JJ dog/NN)))\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IgoDwKk5DJFl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}